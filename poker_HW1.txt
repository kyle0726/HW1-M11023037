import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import precision_score, recall_score, f1_score
from tensorflow.keras.models import Model
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout,LeakyReLU
from tensorflow.keras.optimizers import SGD
from matplotlib import pyplot as plt
from tensorflow.keras import backend as K 
import keras
from keras.utils import to_categorical
import talos

def mean_absolute_error(y_true, y_pred):
    return K.mean(K.abs(y_pred - y_true), axis=-1)

def mean_absolute_percentage_error(y_true, y_pred):
    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),
                                            K.epsilon(),
                                            None))
    return 100. * K.mean(diff, axis=-1)

def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true))) 
def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))
def normal(lt):
    for i in lt.columns:
        if((i == 'S1') or (i == 'CLASS')):
            continue
        lt[i] = (lt[i] - lt[i].min())/(lt[i] - lt[i].min()).max()
    return lt

poker = pd.read_csv('poker-hand-testing.data', low_memory=False)
poker.columns = ['S1','C1','S2','C2','S3','C3','S4','C4','S5','C5','CLASS']
labelencoder = LabelEncoder()
poker['S1'] = labelencoder.fit_transform(poker['S1'])
regression_y = poker['CLASS']
regression_x = poker.drop(columns=['CLASS'])
train_rx, test_rx, train_ry, test_ry = train_test_split(regression_x, regression_y,
                                                    test_size=0.33, 
                                                    random_state=0)
test_ry = test_ry.values
test_ry = [float(test_ry[i]) for i in range(len(test_ry))]
classification_y = poker['S1']
classification_x = poker.drop(columns=['S1'])
train_cx, test_cx, train_cy, test_cy = train_test_split(classification_x, classification_y,
                                                    test_size=0.33, 
                                                    random_state=0)
test_cy = test_cy.values
test_cy = [float(test_cy[i]) for i in range(len(test_cy))]
def talos_regression_best(x_train, y_train, x_val, y_val, params):
    model = Sequential()
    model.add(Dense(256, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='relu'))
#     sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    
    model.compile(loss = 'mse', optimizer = params['optimizer'], metrics = ['mae'])
    out = model.fit(x_train, y_train,
                              epochs=params['epochs'],  
                              batch_size=params['batch_size'],
                              validation_split=0.1)
    return out,model

def build_regression_model(x, y):
    model = Sequential()
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
#     model.add(Dense(128, activation='relu'))
#     model.add(Dropout(0.5))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='relu'))
#     sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    
    model.compile(loss = 'mae', optimizer = 'SGD', metrics = ['mae'])
    model_result = model.fit(x, y,
                              epochs=200,  
                              batch_size=32,
                              validation_split=0.1)
    loss,mae = model.evaluate(x, y, verbose=1)
    print(mae)
    print('Test loss:', loss)
    print('Test accuracy:', mae)
    plt.plot(model_result.history['loss'])
    plt.plot(model_result.history['mae'])
    plt.plot(model_result.history['val_loss'])
    plt.plot(model_result.history['val_mae'])
    plt.title('model accuracy')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['loss', 'mae','val_loss', 'val_mae'], loc='upper left') 
    plt.show()
    return model
modelr = build_regression_model(train_rx, train_ry)


pred_ry = modelr.predict(test_rx)
pred_ry = pred_ry.flatten()
pred_train_ry = modelr.predict(train_rx)
pred_train_ry = pred_train_ry.flatten()
rain_ry2 = train_ry.copy()
train_ry2 = train_ry.values
train_ry2 = [float(train_ry2[i]) for i in range(len(train_ry2))]
mae = mean_absolute_error(test_ry, pred_ry)
mape = mean_absolute_percentage_error(test_ry, pred_ry)
rmse = root_mean_squared_error(test_ry, pred_ry)
print(f'test predict: mae:{mae}, mape:{mape}, rmse:{rmse}')
mae_train = mean_absolute_error(train_ry2, pred_train_ry)
mape_train = mean_absolute_percentage_error(train_ry2, pred_train_ry)
rmse_train = root_mean_squared_error(train_ry2, pred_train_ry)
print(f'train predict: mae:{mae_train}, mape:{mape_train}, rmse:{rmse_train}')
def talos_best(x_train, y_train, x_val, y_val, params):
    model = Sequential()
    model.add(Dense(256, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='relu'))
    model.compile(loss=keras.losses.categorical_crossentropy,
    optimizer=params['optimizer'], metrics = ['accuracy',recall_m,precision_m,f1_m])

    out = model.fit(x_train, y_train,
                              epochs=params['epochs'],  
                              batch_size=params['batch_size'],
                              validation_data=(x_val, y_val))
    return out,model

def build_classification_model(x, y):
    model = Sequential()
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(1, activation='relu'))
#     sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)

    model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adam(lr=0.001), metrics = ['accuracy',recall_m,precision_m,f1_m])

    
    model_result = model.fit(x, y,
                              epochs=200,  
                              batch_size=32,
                              validation_split=0.33)
    score = model.evaluate(x, y, verbose=1)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])
    plt.plot(model_result.history['accuracy'])
    plt.plot(model_result.history['val_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left') 
    plt.show()
    return model
test_cy = to_categorical(test_cy)
train_cy = to_categorical(train_cy)

classification_y =  to_categorical(classification_y)
modelr = build_classification_model(train_rx, train_ry)